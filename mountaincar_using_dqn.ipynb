{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mountain Car- v0 solution using action-value neural network function approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# One hidden layer function approximator\n",
    "class action_value_function(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(action_value_function, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            torch.nn.Linear(2, 100, bias = False),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(20, 4),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 3, bias = False),\n",
    "#             torch.nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.block(input)\n",
    "    \n",
    "# I tried different weight initializations but found they did not perform well.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instaciate action_value_function class\n",
    "q_hat = action_value_function()\n",
    "# q_hat.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "done = False\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.99\n",
    "EPISODES = 25000\n",
    "SHOW_EVERY = 300\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "DISCRETE_OS_SIZE = [20, 20] #* len(env.observation_space.high)\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: [-1.2  -0.07] to [0.6  0.07] | Number of action values: 3\n"
     ]
    }
   ],
   "source": [
    "# Test number of actions and other parameters\n",
    "print('observation_space:',env.observation_space.low,'to',env.observation_space.high,'| Number of action values:',env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for discretization of observation state space\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
    "\n",
    "# Sampled version of q value table\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(q_hat.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "epsilon = 0.3\n",
    "curr_high_pos = 0\n",
    "curr_low_pos = 0\n",
    "successes = 0\n",
    "tot_reward = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | RENDERING | #################################################\n",
      "Episode: 0 | Average Loss: 0.6572934195399285 | Epsilon: 0.3\n",
      "Episode: 100 | Average Loss: 91.24136128859058 | Epsilon: 0.3\n",
      "Episode: 200 | Average Loss: 138.94484069248472 | Epsilon: 0.3\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 300 | Average Loss: 131.32130029336025 | Epsilon: 0.3\n",
      "Episode: 400 | Average Loss: 143.99756023009036 | Epsilon: 0.3\n",
      "Episode: 500 | Average Loss: 155.47570345211935 | Epsilon: 0.3\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 600 | Average Loss: 140.07814855983068 | Epsilon: 0.3\n",
      "Episode: 700 | Average Loss: 132.555411678661 | Epsilon: 0.3\n",
      "Episode: 800 | Average Loss: 136.43251731984927 | Epsilon: 0.3\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 900 | Average Loss: 135.0281351362912 | Epsilon: 0.3\n",
      "Episode: 1000 | Average Loss: 136.395744865626 | Epsilon: 0.3\n",
      "Episode: 1100 | Average Loss: 148.39838880886654 | Epsilon: 0.3\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 1200 | Average Loss: 149.08837957244288 | Epsilon: 0.3\n",
      "Episode: 1300 | Average Loss: 185.91317724499186 | Epsilon: 0.3\n",
      "Episode: 1400 | Average Loss: 182.51057440147113 | Epsilon: 0.28529701496999993\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 1500 | Average Loss: 180.66571303587628 | Epsilon: 0.252882958015178\n",
      "Episode: 1600 | Average Loss: 172.76540479617196 | Epsilon: 0.20892396541487201\n",
      "Episode: 1700 | Average Loss: 143.25155449463304 | Epsilon: 0.16747984156435905\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 1800 | Average Loss: 197.96018591286906 | Epsilon: 0.11203928413628074\n",
      "Episode: 1900 | Average Loss: 200.16129262401003 | Epsilon: 0.07420160576816046\n",
      "Episode: 2000 | Average Loss: 239.68607576128042 | Epsilon: 0.0462665855381534\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 2100 | Average Loss: 208.48381403526412 | Epsilon: 0.030951039090473\n",
      "Episode: 2200 | Average Loss: 190.19891646050615 | Epsilon: 0.021339165607624708\n",
      "Episode: 2300 | Average Loss: 248.64100698720173 | Epsilon: 0.012781221413662938\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 2400 | Average Loss: 224.82290725082004 | Epsilon: 0.008380125670067795\n",
      "Episode: 2500 | Average Loss: 245.6740261636013 | Epsilon: 0.005070026873766029\n",
      "Episode: 2600 | Average Loss: 251.74488965090936 | Epsilon: 0.002917064044331962\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 2700 | Average Loss: 239.12641185315854 | Epsilon: 0.0017297211002931094\n",
      "Episode: 2800 | Average Loss: 254.415419375792 | Epsilon: 0.0010154099083654318\n",
      "Episode: 2900 | Average Loss: 227.49679806707988 | Epsilon: 0.0006143291551926558\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 3000 | Average Loss: 237.07036849775136 | Epsilon: 0.00037921933079383105\n",
      "Episode: 3100 | Average Loss: 265.067819152668 | Epsilon: 0.00021818564325530284\n",
      "Episode: 3200 | Average Loss: 195.79707079718384 | Epsilon: 0.00014305542879724\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 3300 | Average Loss: 212.31704807662206 | Epsilon: 9.009968962841314e-05\n",
      "Episode: 3400 | Average Loss: 193.82093981674842 | Epsilon: 6.0274147817301916e-05\n",
      "Episode: 3500 | Average Loss: 238.04879519162577 | Epsilon: 3.646622512101234e-05\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 3600 | Average Loss: 219.1886484566572 | Epsilon: 2.3199283153525776e-05\n",
      "Episode: 3700 | Average Loss: 255.6297516031356 | Epsilon: 1.3756396490608342e-05\n",
      "Episode: 3800 | Average Loss: 223.87808173271165 | Epsilon: 8.577462554086485e-06\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 3900 | Average Loss: 216.03066189630252 | Epsilon: 5.456857184166162e-06\n",
      "Episode: 4000 | Average Loss: 219.9824435293131 | Epsilon: 3.506639737464523e-06\n",
      "Episode: 4100 | Average Loss: 177.24793061029712 | Epsilon: 2.393473065852427e-06\n",
      "################################################## | RENDERING | #################################################\n",
      "Episode: 4200 | Average Loss: 279.4711129078966 | Epsilon: 1.3633251364062354e-06\n",
      "Episode: 4300 | Average Loss: 233.03550486167325 | Epsilon: 8.586543879220174e-07\n"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "import random\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "   \n",
    "    state = torch.from_numpy(env.reset()).float()\n",
    "    done = False\n",
    "    \n",
    "    if episode%SHOW_EVERY == 0:\n",
    "        print('#'*50,'| RENDERING |','#'*49)\n",
    "        RENDER = True\n",
    "    else:\n",
    "        RENDER = False\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        \n",
    "        q = q_hat(state)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = torch.argmax(q).item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        \n",
    "        q_target = q.clone()\n",
    "        q_target = q_target.data\n",
    "        \n",
    "        q_target[action] = reward + DISCOUNT * max(q_hat(next_state).detach())\n",
    "        # Calculate loss\n",
    "        loss = criterion(q, q_target)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        if next_state[0] >= 0.5:\n",
    "            epsilon *= .99\n",
    "            scheduler.step()\n",
    "            break \n",
    "        \n",
    "\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    avg_loss += running_loss\n",
    "    env.close()\n",
    "    if episode%100 == 0:\n",
    "        print('Episode:',episode,'| Average Loss:',avg_loss/100, '| Epsilon:',epsilon)\n",
    "        avg_loss = 0\n",
    "\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your policy\n",
    "env.close()\n",
    "state = torch.from_numpy(env.reset()).float()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    q = q_hat(state)\n",
    "    action = torch.argmax(q).item()\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = torch.from_numpy(next_state).float()\n",
    "    state = next_state\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
